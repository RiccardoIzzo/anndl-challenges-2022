{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNDL Homework 1 - Best Model\n",
    "**Team:** MercyMain\n",
    "\n",
    "**Team Members:**\n",
    "- Azimi Arya\n",
    "- Belotti Ottavia\n",
    "- Izzo Riccardo\n",
    "\n",
    "\n",
    "The best model consists in an ensemble of two models both based on _Xception_. \n",
    "The first one is a straight forward transfer learning, while the second exploit the CutOut method for data augmentation and a periodic scheduling of the learning rate. \n",
    "The scheduler keeps the learning rate constant for 5 epochs, then it performs an exponential decay for the following 10 epochs, finally it goes back to the initial learning rate and restart the cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "\n",
    "# Transfer learning with Xception\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfk.__version__\n",
    "\n",
    "# Random seed for reproducibility\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Method for checkpoints save and callbacks \n",
    "\n",
    "def create_folders_and_callbacks(model_name):\n",
    "\n",
    "    exps_dir = './Challenge_CheckPoints'\n",
    "    if not os.path.exists(exps_dir):\n",
    "        os.makedirs(exps_dir)\n",
    "\n",
    "    now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    # Model checkpoint\n",
    "\n",
    "    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp'),\n",
    "                                                       save_weights_only=False,      \n",
    "                                                       save_best_only=True)         \n",
    "    callbacks.append(ckpt_callback)\n",
    "\n",
    "    # Visualize Learning on Tensorboard\n",
    "\n",
    "    tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "    if not os.path.exists(tb_dir):\n",
    "        os.makedirs(tb_dir)\n",
    "\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                                 profile_batch=0,\n",
    "                                                 histogram_freq=1) \n",
    "    callbacks.append(tb_callback)\n",
    "\n",
    "    # Early Stopping\n",
    "\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutout preprocessing function\n",
    "def eraser(input_img):\n",
    "    p=0.5\n",
    "    s_l=0.02\n",
    "    s_h=0.4\n",
    "    r_1=0.3\n",
    "    r_2=1/0.3\n",
    "    v_l=0\n",
    "    v_h=255\n",
    "    pixel_level=False\n",
    "    \n",
    "    img_h, img_w, img_c = input_img.shape\n",
    "    p_1 = np.random.rand()\n",
    "\n",
    "    if p_1 > p:\n",
    "        return input_img\n",
    "\n",
    "    while True:\n",
    "        s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "        r = np.random.uniform(r_1, r_2)\n",
    "        w = int(np.sqrt(s / r))\n",
    "        h = int(np.sqrt(s * r))\n",
    "        left = np.random.randint(0, img_w)\n",
    "        top = np.random.randint(0, img_h)\n",
    "\n",
    "        if left + w <= img_w and top + h <= img_h:\n",
    "            break\n",
    "\n",
    "    if pixel_level:\n",
    "        c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "    else:\n",
    "        c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "    input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "    return input_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise preprocessing function\n",
    "# Custom function for gaussian noise \n",
    "def add_noise(img):\n",
    "    # Add random noise to an image\n",
    "    VARIABILITY = 25\n",
    "    deviation = VARIABILITY*random.random()\n",
    "    noise = np.random.normal(0, deviation, img.shape)\n",
    "    img += noise\n",
    "    np.clip(img, 0., 255.)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset folders\n",
    "training_dir = \"/kaggle/input/dataset-homework-1/training_data_final/\"\n",
    "\n",
    "split_seed = 123  # Splitting seed needed to avoid overlap in training and validation set\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation for training set for First Model\n",
    "\n",
    "train_data_gen_V1 = ImageDataGenerator(\n",
    "    validation_split=0.1,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(0.3, 1.7),\n",
    "    rotation_range=180,\n",
    "    height_shift_range=0.3,\n",
    "    width_shift_range=0.3,\n",
    "    fill_mode='reflect',\n",
    "    channel_shift_range=0.5,\n",
    "    preprocessing_function=add_noise,\n",
    "    )\n",
    "\n",
    "valid_data_gen_V1 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Data load from directory and split in validation and training\n",
    "\n",
    "training_set_V1 = train_data_gen_V1.flow_from_directory(\n",
    "    directory=training_dir,\n",
    "    target_size=(299, 299),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "validation_set_V1 = valid_data_gen_V1.flow_from_directory(\n",
    "    directory=training_dir,\n",
    "    target_size=(299, 299),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "from collections import Counter\n",
    "counter_V1 = Counter(training_set_V1.classes)\n",
    "weights_V1 = {}\n",
    "\n",
    "# This weighting tells the model to \"pay more attention\" to samples from an under-represented class when passed to the mpdel's fit function (e.g. Species1 and Species6)\n",
    "for i in counter_V1.items():\n",
    "    weights_V1[i[0]] = 1/i[1]*training_set_V1.samples/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training set for Second Model\n",
    "\n",
    "train_data_gen_V2 = ImageDataGenerator(\n",
    "    validation_split=0.1,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(0.3, 1.7),\n",
    "    rotation_range=180,\n",
    "    height_shift_range=0.3,\n",
    "    width_shift_range=0.3,\n",
    "    fill_mode='nearest',\n",
    "    channel_shift_range=0.5,\n",
    "    preprocessing_function=eraser,\n",
    "    )\n",
    "\n",
    "valid_data_gen_V2 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Data load from directory and split in validation and training\n",
    "\n",
    "training_set_V2 = train_data_gen_V2.flow_from_directory(\n",
    "    directory=training_dir,\n",
    "    target_size=(299, 299),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "validation_set_V2 = valid_data_gen_V2.flow_from_directory(\n",
    "    directory=training_dir,\n",
    "    target_size=(299, 299),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "from collections import Counter\n",
    "counter_V2 = Counter(training_set_V2.classes)\n",
    "weights_V2 = {}\n",
    "\n",
    "for i in counter_V2.items():\n",
    "    weights_V2[i[0]] = 1/i[1]*training_set_V2.samples/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler_fit(epoch, lr):\n",
    "    base_rate = 1e-3\n",
    "    if epoch % 15 < 5:\n",
    "        return base_rate\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.01)\n",
    "\n",
    "\n",
    "def scheduler_tuning(epoch, lr):\n",
    "    base_rate = 1e-5\n",
    "    if epoch % 15 < 5:\n",
    "        return base_rate\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model: Transfer Learning with Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "input_shape = (299, 299, 3)\n",
    "\n",
    "# Import Xception pretrained application without the FC part\n",
    "\n",
    "base_model = Xception(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=input_shape,\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "# Freeze every layer in the net\n",
    "base_model.trainable = False\n",
    "\n",
    "def build_model(input_shape):\n",
    "    # Use the Xception feature extraction part\n",
    "    x = base_model.output\n",
    "    \n",
    "    global_average = tfkl.GlobalAveragePooling2D()(x)\n",
    "    dense1 = tfkl.Dense(units=256, activation='gelu', kernel_initializer='he_uniform', input_dim=input_shape)(global_average)\n",
    "    dp1 = tfkl.Dropout(0.3)(dense1)\n",
    "    dense2 = tfkl.Dense(units=128, activation='gelu', kernel_initializer='he_uniform')(dp1)\n",
    "    dp2 = tfkl.Dropout(0.3)(dense2)\n",
    "    output_layer = tfkl.Dense(units=8, activation='softmax', kernel_initializer='he_uniform')(dp2)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=base_model.input, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), \n",
    "                  optimizer=tfk.optimizers.Adam(), \n",
    "                  metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(input_shape)\n",
    "model.summary() \n",
    "    \n",
    "    \n",
    "# Create folders and callbacks and fit\n",
    "locals_callbacks = create_folders_and_callbacks(model_name='transfer_Xception_0')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "\ttraining_set_V1,\n",
    "\tepochs=epochs,\n",
    "\tvalidation_data=validation_set_V1,\n",
    "\tcallbacks=locals_callbacks,\n",
    "    class_weight=weights_V1\n",
    ").history\n",
    "\n",
    "#----------------------\n",
    "# Fine tuning part\n",
    "\n",
    "model.trainable = True\n",
    "\n",
    "# Recompile the model with lower learning rate\n",
    "model.compile(loss=tfk.losses.CategoricalCrossentropy(), \n",
    "              optimizer=tfk.optimizers.Adam(1e-4), \n",
    "              metrics='accuracy')\n",
    "\n",
    "# Callbacks\n",
    "locals_callbacks1 = create_folders_and_callbacks(model_name='transfer_Xception_1')\n",
    "\n",
    "# Retrain the model\n",
    "history = model.fit(\n",
    "\ttraining_set_V1,\n",
    "\tepochs=epochs,\n",
    "\tvalidation_data=validation_set_V1,\n",
    "\tcallbacks=locals_callbacks1,\n",
    "    class_weight=weights_V1\n",
    ").history\n",
    "\n",
    "# Save model\n",
    "model.save(\"/kaggle/output/working/Transfer_Xception_Save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model: Transfer Learning with Xception, CutOut and Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_shape = (299, 299, 3)\n",
    "\n",
    "# Import Xception pretrained application without the FC part\n",
    "\n",
    "base_model = Xception(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=input_shape,\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "# Freeze every layer in the net\n",
    "base_model.trainable = False\n",
    "\n",
    "def build_model(input_shape):\n",
    "    # Use the Xception feature extraction part\n",
    "    x = base_model.output\n",
    "    \n",
    "    #Build a custom FC part\n",
    "    global_average = tfkl.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    dense1 = tfkl.Dense(units=256, activation='gelu', kernel_initializer='he_uniform')(global_average)\n",
    "    bn5 = tfkl.BatchNormalization()(dense1)\n",
    "    dp3 = tfkl.Dropout(0.3)(bn5)\n",
    "    dense2 = tfkl.Dense(units=128, activation='gelu', kernel_initializer='he_uniform')(dp3)\n",
    "\n",
    "    bn6 = tfkl.BatchNormalization()(dense1)\n",
    "\n",
    "    dp4 = tfkl.Dropout(0.3)(bn6)\n",
    "\n",
    "    output_layer = tfkl.Dense(units=8, activation='softmax', kernel_initializer='he_uniform',name='Output')(dp4)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=base_model.input, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), \n",
    "                  optimizer=tfk.optimizers.Adam(), \n",
    "                  metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(input_shape)\n",
    "model.summary() \n",
    "    \n",
    "    \n",
    "# Create folders and callbacks and fit\n",
    "locals_callbacks = create_folders_and_callbacks(model_name='transfer_Xception_0_V2')\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler_fit)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "\ttraining_set_V2,\n",
    "\tepochs=epochs,\n",
    "\tvalidation_data=validation_set_V2,\n",
    "\tcallbacks=[locals_callbacks, lr_callback],\n",
    "    class_weight=weights_V2\n",
    ").history\n",
    "\n",
    "#---------------------------\n",
    "# Fine tuning part\n",
    "\n",
    "# Unfreeze all the layers in the Xception\n",
    "model.trainable = True\n",
    "\n",
    "# Recompile the model\n",
    "model.compile(loss=tfk.losses.CategoricalCrossentropy(), \n",
    "              optimizer=tfk.optimizers.Adam(), \n",
    "              metrics='accuracy')\n",
    "\n",
    "# Callbacks\n",
    "locals_callbacks1 = create_folders_and_callbacks(model_name='transfer_Xception_1_V2')\n",
    "# Scheduler with lower intial learning rate for tuning\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler_tuning)\n",
    "\n",
    "# Retrain the model\n",
    "history1 = model.fit(\n",
    "\ttraining_set_V2,\n",
    "\tepochs=epochs,\n",
    "\tvalidation_data=validation_set_V2,\n",
    "\tcallbacks=[locals_callbacks1, lr_callback],\n",
    "    class_weight=weights_V2\n",
    ").history\n",
    "\n",
    "# Save model\n",
    "model.save(\"/kaggle/output/working/Transfer_Xception_V2_Save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "Once the models have been trained, the ensemble is done at prediction time as follow, for each input:\n",
    "1. Get the class predictions/probabilities from both models\n",
    "2. Compute the average of the scores between the 2 classification output vectors, class-wise\n",
    "3. Consider the new averaged vector as the classification output, hence extract the predicted class for the image by choosing the most likely one (i.e. _argmax_)\n",
    "\n",
    "In the code snippet below, we insert the procedure done in the `Model.py` file placed in the submission folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.py\n",
    "class model:\n",
    "    def __init__(self, path):\n",
    "        self.model_0 = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel', 'Xception_0'))\n",
    "        self.model_1 = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel', 'Xception_lr_sched_cutout'))\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Insert your preprocessing here\n",
    "        X = X /255.\n",
    "\n",
    "        X = tf.image.resize(\n",
    "            X,\n",
    "            (299, 299),\n",
    "            method='nearest',\n",
    "            preserve_aspect_ratio=False,\n",
    "            antialias=False\n",
    "        )\n",
    "\n",
    "        out_0 = self.model_0.predict(X)\n",
    "        out_1 = self.model_1.predict(X)\n",
    "\n",
    "        # Initialize averaged prediction matrix\n",
    "        out_avg = np.empty(shape=out_0.shape)\n",
    "\n",
    "        # Compute avg prediction scores between the 2 available models\n",
    "        for i in range(len(out_0)):\n",
    "            for j in range(len(out_0[i])):\n",
    "                out_avg[i, j] = (out_0[i, j] + out_1[i, j]) / 2\n",
    "\n",
    "        # Get best class prediction for each image\n",
    "        out = tf.argmax(out_avg, axis=-1)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
