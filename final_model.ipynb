{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNDL Homework 2 - Best Model\n",
    "**Team**: MercyMain\n",
    "\n",
    "**Team Members**:\n",
    "\n",
    "- Azimi Arya\n",
    "- Belotti Ottavia\n",
    "- Izzo Riccardo\n",
    "\n",
    "The best model consists in an ensemble of three models, two of them consist in 1DCNNs while the last one is a BiLSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "\n",
    "dataset = np.load('/kaggle/input/dataset-homework-2/x_train.npy')\n",
    "outputs = np.load('/kaggle/input/dataset-homework-2/y_train.npy')\n",
    "\n",
    "print(\"Mean: \" + str(dataset.mean()))\n",
    "print(\"Standard Variation: \" + str(dataset.std()))\n",
    "\n",
    "label_mapping = {\n",
    "    \"Wish\": 0,\n",
    "    \"Another\": 1,\n",
    "    \"Comfortably\": 2,\n",
    "    \"Money\": 3,\n",
    "    \"Breathe\": 4,\n",
    "    \"Time\": 5,\n",
    "    \"Brain\": 6,\n",
    "    \"Echoes\": 7,\n",
    "    \"Wearing\": 8,\n",
    "    \"Sorrow\": 9,\n",
    "    \"Hey\": 10,\n",
    "    \"Shine\": 11\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataframe to print and plot dataset's properties\n",
    "dataset_res = dataset.reshape([dataset.shape[0]*dataset.shape[1], dataset.shape[2]])\n",
    "print(\"Dataset reshaped: \" + str(dataset_res.shape))\n",
    "\n",
    "scale_columns = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6']\n",
    "df = pd.DataFrame(dataset_res, columns=scale_columns)\n",
    "out_df = pd.DataFrame(outputs, columns=['target_class'])\n",
    "new_out = pd.DataFrame()\n",
    "\n",
    "# Lengthen the output dataframe to match the input size by repeating the classes\n",
    "# not optimized\n",
    "for i in range(out_df.shape[0]):\n",
    "    for j in range(36):\n",
    "        new_out = new_out.append(out_df.iloc[i], ignore_index=True)\n",
    "        \n",
    "print('df_out.shape ' + str(out_df.shape))\n",
    "print('new_out.shape ' + str(new_out.shape))\n",
    "\n",
    "# Create 12 elements array containing starting and ending index element in dataset corresponding to each class\n",
    "# not optimized\n",
    "classes = []\n",
    "ctr = 0\n",
    "start = 0\n",
    "for i in range(new_out.shape[0]):\n",
    "    if new_out.iloc[i,0] > ctr:\n",
    "        classes.append((start, i-1))\n",
    "        start = i\n",
    "        ctr = new_out.iloc[i,0]\n",
    "classes.append((start, new_out.shape[0]-1))\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distribution over the dataset\n",
    "fig, axs = plt.subplots(2, 3, figsize=(34,22))\n",
    "x = np.arange(0, df.shape[0])\n",
    "\n",
    "def boolean_masking(arr, class_idx=0):\n",
    "    bool_arr = []\n",
    "    for i in arr:\n",
    "        if i >= classes[class_idx][0] and i <= classes[class_idx][1]:\n",
    "            bool_arr.append(True)\n",
    "        else:\n",
    "            bool_arr.append(False)\n",
    "    return bool_arr\n",
    "\n",
    "colors = ['blue', 'orange', 'green', 'red', 'saddlebrown', 'purple']\n",
    "class_colors = ['purple', 'green', 'red', 'royalblue', 'yellow', 'sienna', 'pink', 'lime', 'darkorange', 'aqua', 'gold', 'orchid']\n",
    "\n",
    "for i in range(6):\n",
    "    j = 0 if i <3 else 1\n",
    "    axs[j][i%3].plot(df[scale_columns[i]], label='Feature ' + str(i+1), color=colors[i], linewidth=2)\n",
    "    axs[j][i%3].legend()\n",
    "    for k in range(12):\n",
    "        axs[j][i%3].fill_between(x, -10000, 50000, where=boolean_masking(x, k), color=class_colors[k], alpha=0.3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw heatmap visualization of the correlation matrix\n",
    "corr_matrix = df.corr(method='spearman')\n",
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidth=0.4,\n",
    "            annot_kws={\"size\": 11}, cmap='coolwarm', ax=ax)\n",
    "plt.title(\"Correlation matrix of training set\")\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Standardization for CNNs \n",
    "# (do not run this if building/fitting BiLSTM model)\n",
    "dataset = (dataset - np.mean(dataset)) / np.std(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in training set and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, outputs, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights for BiLSTM model fit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_train), y= y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse labels to categorical values\n",
    "y_train = tfk.utils.to_categorical(y_train)\n",
    "y_test = tfk.utils.to_categorical(y_test)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "classes = y_train.shape[-1]\n",
    "batch_size = 32 # for CNNs - see later for BiLSTM batch size\n",
    "epochs = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model: Best 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1DCNN_classifier(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor\n",
    "    cnn = tfkl.Conv1D(128,3,padding='same',activation='relu')(input_layer)\n",
    "    cnn = tfkl.MaxPooling1D()(cnn)\n",
    "    cnn = tfkl.Conv1D(128,3,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.MaxPooling1D()(cnn)\n",
    "    cnn = tfkl.Conv1D(256,3,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.MaxPooling1D()(cnn)\n",
    "    cnn = tfkl.Conv1D(256,3,padding='same',activation='relu')(cnn)\n",
    "    gap = tfkl.GlobalAveragePooling1D()(cnn)\n",
    "    dropout = tfkl.Dropout(.2, seed=seed)(gap)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
    "    classifier = tfkl.Dense(64, activation='relu')(classifier)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model: Second 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1DCNN_classifier_2(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor\n",
    "    cnn = tfkl.Conv1D(64,3,padding='same',activation='relu')(input_layer)\n",
    "    cnn = tfkl.MaxPooling1D()(cnn)\n",
    "    cnn = tfkl.Conv1D(128,3,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.Conv1D(128,3,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.MaxPooling1D()(cnn)\n",
    "    cnn = tfkl.Conv1D(256,3,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.Conv1D(256,3,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.GlobalAveragePooling1D()(cnn)\n",
    "    cnn = tfkl.Dropout(.25, seed=seed)(cnn)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(128, activation='relu')(cnn)\n",
    "    classifier = tfkl.Dropout(.5, seed=seed)(classifier)\n",
    "    classifier = tfkl.Dense(64, activation='relu')(classifier)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model: BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_BiLSTM_classifier(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor\n",
    "    bilstm = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(input_layer)\n",
    "    bilstm = tfkl.Bidirectional(tfkl.LSTM(128))(bilstm)\n",
    "    dropout = tfkl.Dropout(.5, seed=seed)(bilstm)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
    "    output_layer = tfkl.Dense(12, activation='softmax')(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training first CNN\n",
    "model1 = build_1DCNN_classifier(input_shape, classes)\n",
    "model1.summary()\n",
    "\n",
    "# Train the model\n",
    "history1 = model1.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=7, factor=0.5, min_lr=1e-5)\n",
    "    ]\n",
    ").history\n",
    "\n",
    "model1.save('/kaggle//working/1DCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training second CNN\n",
    "model2 = build_1DCNN_classifier_2(input_shape, classes)\n",
    "model2.summary()\n",
    "\n",
    "# Train the model\n",
    "history2 = model2.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=7, factor=0.5, min_lr=1e-5)\n",
    "    ]\n",
    ").history\n",
    "\n",
    "model2.save('/kaggle/working/1DCNN_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training BiLSTM - be aware that BiLSTM doesn't use the standardized training set\n",
    "batch_size = 128\n",
    "\n",
    "model3 = build_BiLSTM_classifier(input_shape, classes)\n",
    "model3.summary()\n",
    "\n",
    "# Train the model\n",
    "history3 = model3.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    class_weights=class_weights,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=7, factor=0.5, min_lr=1e-5)\n",
    "    ]\n",
    ").history\n",
    "\n",
    "model3.save('/kaggle/working/BiLSTM')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "Once the models have been trained, the ensemble is done at prediction time as follow, for each input:\n",
    "1. Get the class predictions/probabilities from all the models\n",
    "2. Compute the average of the scores between the 3 classification output vectors, class-wise\n",
    "3. Consider the new averaged vector as the classification output, hence extract the predicted class for the image by choosing the most likely one (i.e. _argmax_)\n",
    "\n",
    "In the code snippet below, we insert the procedure done in the `Model.py` file placed in the submission folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self, path):\n",
    "        self.model_1 = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel', 'firstCnn'))\n",
    "        self.model_2 = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel', 'secondCnn'))\n",
    "        self.model_3 = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel', 'bilstm'))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Insert your preprocessing here\n",
    "        # Standardization - mean and std come from the training dataset\n",
    "        mean = 25.295960919260704\n",
    "        std = 658.4301638513654\n",
    "        X_std = (X - mean) / std\n",
    "\n",
    "        # Only the CNNs use standardized data\n",
    "        out_1 = self.model_1.predict(X_std)\n",
    "        out_2 = self.model_2.predict(X_std)\n",
    "        out_3 = self.model_3.predict(X)\n",
    "\n",
    "        # Initialize averaged prediction matrix\n",
    "        out_avg = np.empty(shape=out_1.shape)\n",
    "\n",
    "        # Compute avg prediction scores between the 2 available models\n",
    "        for i in range(len(out_1)):\n",
    "            for j in range(len(out_1[i])):\n",
    "                out_avg[i, j] = (out_1[i, j] + out_2[i, j] + out_3[i, j]) / 3\n",
    "\n",
    "        # Get best class prediction for each image\n",
    "        out = tf.argmax(out_avg, axis=-1)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0 (v3.10.0:b494f5935c, Oct  4 2021, 14:59:20) [Clang 12.0.5 (clang-1205.0.22.11)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
